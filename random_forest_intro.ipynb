{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest Introduction\n",
    "\n",
    "John Lawson\n",
    "Class 496\n",
    "\n",
    "Random forests (RFs) are machine-learning (ML) models that are ostensibly large decision trees. Many of the decision trees are run with slightly different numbers to simulate \"alternate realities\" to reflect our uncertainty about the true state of the system (system = lake-breeze front, for our project).\n",
    "\n",
    "We will use an introductory form of the RF, a *classifier*. Let's say we want to predict the species of a flower from characteristics of the petal itself. This will draw from a famous dataset, \"Iris\", that is a good way to test any ML model to become familiar with it. In our case, the python package we will use for ML, `scikit learn`, has this dataset baked in, making it easy to load when testing new ML systems (the RF is only one of many!).\n",
    "\n",
    "OK. Bear with the AI jargon. It's coming.\n",
    "\n",
    "This RF Classifier takes input variables (like the length and width of a flower's petal) and predicts from this which class (like a species) the petal is from. Because this RF may only may a choice from the classes (\"labels\"), it cannot imply likelihood that its choice is correct. Let's jump for a minute back to meteorology. Swap species for weather events occurring, and petal width/length for sensible-weather variables (temperature, moisture, etc.), and you can see where we are going: __We want our RF to predict one of two classes (a LBF will pass; a LBF will not pass) using available weather data.__ Here, \"available\" is down to you as the student and researcher.\n",
    "\n",
    "Anyway, back to our example so we get an initial feel."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels=['setosa', 'versicolor', 'virginica']\n",
      "features=['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# The classes, or labels, are the species of plant. It's our predictand.\n",
    "# Here, they are called \"target names\" -- see what I mean about jargon?!\n",
    "labels = list(iris.target_names)\n",
    "\n",
    "# The features (or predictors) are the variables we are learning from to\n",
    "# determine the system's class (i.e., the species of plant).\n",
    "features = list(iris.feature_names)\n",
    "\n",
    "# Fun trick for checking your Python code:\n",
    "# This is called f-formatting\n",
    "# Use an equals sign after a variable to have its name printed\n",
    "print(f\"{labels=}\")\n",
    "print(f\"{features=}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's look at the data. We'll use the package `pandas` to interact with the dataset. It's like breeding a spreadsheet (like Excel) with `numpy`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1    2    3\n",
      "0    5.1  3.5  1.4  0.2\n",
      "1    4.9  3.0  1.4  0.2\n",
      "2    4.7  3.2  1.3  0.2\n",
      "3    4.6  3.1  1.5  0.2\n",
      "4    5.0  3.6  1.4  0.2\n",
      "..   ...  ...  ...  ...\n",
      "145  6.7  3.0  5.2  2.3\n",
      "146  6.3  2.5  5.0  1.9\n",
      "147  6.5  3.0  5.2  2.0\n",
      "148  6.2  3.4  5.4  2.3\n",
      "149  5.9  3.0  5.1  1.8\n",
      "\n",
      "[150 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import numpy as np\n",
    "df_features = pd.DataFrame(iris.data)\n",
    "print(df_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, we need to make this more organised. Here, columns 0-3 are the features, and each row is a different sample. We can leave the rows un-named and their order is arbitrary.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                  5.1               3.5                1.4               0.2\n",
      "1                  4.9               3.0                1.4               0.2\n",
      "2                  4.7               3.2                1.3               0.2\n",
      "3                  4.6               3.1                1.5               0.2\n",
      "4                  5.0               3.6                1.4               0.2\n",
      "..                 ...               ...                ...               ...\n",
      "145                6.7               3.0                5.2               2.3\n",
      "146                6.3               2.5                5.0               1.9\n",
      "147                6.5               3.0                5.2               2.0\n",
      "148                6.2               3.4                5.4               2.3\n",
      "149                5.9               3.0                5.1               1.8\n",
      "\n",
      "[150 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df_features.columns = features\n",
    "print(df_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see what each sample's species turned out to be (this is *truth*, and we assume the investigator logging the data is 100% sure of the species):\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Species\n",
      "0          0\n",
      "1          0\n",
      "2          0\n",
      "3          0\n",
      "4          0\n",
      "..       ...\n",
      "145        2\n",
      "146        2\n",
      "147        2\n",
      "148        2\n",
      "149        2\n",
      "\n",
      "[150 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_labels = pd.DataFrame(iris.target)\n",
    "df_labels.columns = [\"Species\"]\n",
    "print(df_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rename the numbers with their label names just for humans."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Species\n",
      "0       setosa\n",
      "1       setosa\n",
      "2       setosa\n",
      "3       setosa\n",
      "4       setosa\n",
      "..         ...\n",
      "145  virginica\n",
      "146  virginica\n",
      "147  virginica\n",
      "148  virginica\n",
      "149  virginica\n",
      "\n",
      "[150 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "label_dict = {n:labels[n] for n in (0,1,2)}\n",
    "df_labels_named = df_labels.replace({\"Species\":label_dict})\n",
    "print(df_labels_named)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So there's our data looking quite neat. It's time to start our machine learning!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "# Split the dataset up between training and testing.\n",
    "# We train the RF with half of the dataset, showing it properties of different species.\n",
    "# Then we want to test it like a student, and hold back the answers (labels) so we can grade it!\n",
    "\n",
    "# I'll use f_ for features (variables, predictors) and\n",
    "# l_ for labels (species, classes, predictands)\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "f_train, f_test, l_train, l_test = tts(df_features.values,df_labels,test_size=0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "# Have a play with the data here:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importantly, this function `train_test_split` (which I aliases to `tts` to make it easier to type repeatedly) will shuffle the datasets in the same way before slicing off half (as we picked a value of 0.5 for test size). This means we should be careful to preserve the order of outputs. This is a frequent place for trip-ups, in my experience, including re-assigning class names (e.g., species names) to integers.\n",
    "\n",
    "Now let's train our forest of trees! (That's a tautology, surely?)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\n",
      "0  2\n",
      "1  2\n",
      "2  1\n",
      "3  2\n",
      "4  1\n",
      "5  0\n",
      "6  2\n",
      "7  0\n",
      "8  0\n",
      "9  1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "import numpy as np\n",
    "\n",
    "# Time to turn to the official online (or locally installed) documentation\n",
    "# What are the estimators? How do we pick this one number?\n",
    "rfc = RFC(n_estimators=100)\n",
    "\n",
    "rfc.fit(f_train,np.ravel(l_train))\n",
    "fcst = rfc.predict(f_test)\n",
    "\n",
    "# Make this easier to read with DataFrames and first 10 entries\n",
    "print(pd.DataFrame(fcst).head(10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the predicted species for each sample. Shall we look at the answers for the same samples? (The ordering is important but positioning is arbitrary.)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\n",
      "0  2\n",
      "1  1\n",
      "2  1\n",
      "3  2\n",
      "4  1\n",
      "5  0\n",
      "6  2\n",
      "7  0\n",
      "8  0\n",
      "9  1\n"
     ]
    }
   ],
   "source": [
    "# print(pd.DataFrame({\"Forecast\":fcst,\"Actual\":l_test.values}))\n",
    "print(pd.DataFrame(l_test.values).head(10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "How did it do? Use your eyes first with any data analysis. It's our first defence against rubbish.\n",
    "\n",
    "Of course, it's important to evaluate the performance with maths rather than just by eye, and here we'll look at the R-squared correlation (remember evaluating lines of best fit?) to keep things simple. First we need to move back to numbers to score the forecast."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8471170646476412\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.r2_score(l_test.values,fcst))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "92% accurate?! That's pretty sweet.\n",
    "\n",
    "Already you can see how scarily easy (relatively!) this appears, but how difficult it gets making sense of the various dials you can tune. (Look at that documentation for this `RFC` function!) Not to mention we haven't talked about how these RFs __really__ work. But one step at a time.\n",
    "\n",
    "\n",
    "Brainstorm what else you need to know so you're not just using the RF like a 6-year-old using a microwave. Also think about:\n",
    "\n",
    "- It will be important not only to see tables printed out as a sanity check (see above), but also to visualise your data in graphical form. Take a look on the scikit-learn.org examples (such as `plot_forest_iris`) to see different ways people like to see the results. It can be quite confusing as a meteorologist wading into this.\n",
    "- We don't just have four variables. We will have a lot to sort through. How do we rank the importance of each one to our prediction accuracy? Why, it's literally *feature importance*, and it will help us chuck out variables we care less about.\n",
    "- How are we getting our features and labels? There is an online repository for weather observations called MesoWest (`mesowest.utah.edu`), but I won't expect you to spend weeks writing a Python script to download these variables. I will give you a dataset in time that will have lots of observations from the Chicago area. The time period will depend on the observation stations we have. We probably want as large a dataset as possible (as long as the forecast doesn't take too long to do)\n",
    "- What is our \"label\"? It is \"yes/no\": a LBF will pass, or it won't. We are missing a vital variable that cannot be measured with an instrument: the LBF passage itself. I will have a script for you coming up that will take some observed variables (dry-bulb and dewpoint temperatures; wind components; pressure; large-scale wind strength) to create a best-guess dataset of which days in our training time period we can use to train our RF.\n",
    "- Don't forget time of day, latitude/longitude of observation station, etc. will be important. I'll show you a funky way of getting time of day and day of year into the weather model. (Because it doesn't work if you just enter numbers from 0 to 23! Think about why when dealing with time-of-day, which is cyclical...)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
